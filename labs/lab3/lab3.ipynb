{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from labs.helpers import read_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_dir = \"../../data/train\"\n",
    "classes = [\"field\", \"water\"]\n",
    "file_pattern = \"*{}*.jpg\"\n",
    "standard_shape = (50, 50, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset, features = read_dataset(data_dir, classes, file_pattern, standard_shape)\n",
    "train_df = dataset.sample(frac=0.8, random_state=18)\n",
    "test_df = dataset.drop(train_df.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(18, 7501)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 7501)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class LinearPerceptron:\n",
    "\n",
    "    __features = None\n",
    "    __class_to_number = None\n",
    "    __number_to_class = None\n",
    "    __weights = None\n",
    "    __old_weights_delta = None\n",
    "\n",
    "    def __init__(self, learning_rate=1, moment=0.8, max_epoch=10000):\n",
    "        self.__learning_rate = learning_rate\n",
    "        self.__moment = moment\n",
    "        self.__max_epoch = max_epoch\n",
    "        self.__activation = expit\n",
    "\n",
    "    def fit(self, df, train_features, target):\n",
    "        self.__features = train_features\n",
    "        self.__build_classes_binarizers(df, target)\n",
    "        train_values = self.__build_values(df)\n",
    "        binarized_classes = df[target].apply(lambda class_: self.__class_to_number[class_]).values.astype(\"float32\")\n",
    "        self.__old_weights_delta = np.zeros(train_values.shape[1])\n",
    "        self.__weights = np.full(train_values.shape[1], 0.001)\n",
    "        # inside for operator\n",
    "        for epoch_number in range(self.__max_epoch):\n",
    "            cum_error = 0\n",
    "            for measure_index in range(train_values.shape[0]):\n",
    "                measure = train_values[measure_index]\n",
    "                real_value = binarized_classes[measure_index]\n",
    "                cum_error += self.__learn(measure, real_value)\n",
    "                if np.mean(cum_error) < 0.000000001:\n",
    "                    print(\"End of learn\")\n",
    "                    return\n",
    "\n",
    "    def __build_values(self, df):\n",
    "        measures = df[self.__features].values\n",
    "        bias_neuron_vals = np.ones((measures.shape[0], 1))\n",
    "        values = np.hstack((bias_neuron_vals, measures))\n",
    "        return values\n",
    "\n",
    "    def __learn(self, measure, real_value):\n",
    "        neuron_output = self.__go_forward(measure)\n",
    "        self.__go_backward(measure, neuron_output, real_value)\n",
    "        return self.__go_forward(measure)\n",
    "\n",
    "    def __go_forward(self, measure):\n",
    "        multiplied = np.multiply(measure, self.__weights)\n",
    "        neuron_input = np.sum(multiplied)\n",
    "        neuron_output = self.__activation(neuron_input)\n",
    "        return neuron_output\n",
    "\n",
    "    def __go_backward(self, measure, neuron_output, real_value):\n",
    "        out_neuron_error = (real_value - neuron_output) * self.__activation_derivative(neuron_output)\n",
    "        neuron_activation_derivation = np.multiply(1 - measure, measure)\n",
    "        error_by_weights = out_neuron_error * self.__weights\n",
    "        in_neurons_errors = np.multiply(neuron_activation_derivation, error_by_weights)\n",
    "        gradients = np.multiply(in_neurons_errors, measure)\n",
    "        new_weights_delta = self.__learning_rate * gradients + self.__old_weights_delta * self.__moment\n",
    "        self.__weights += new_weights_delta\n",
    "        self.__old_weights_delta = new_weights_delta\n",
    "\n",
    "    def __build_classes_binarizers(self, df, target):\n",
    "        self.__class_to_number = dict()\n",
    "        self.__number_to_class = dict()\n",
    "        classes_list = df[target].unique()\n",
    "        for k, v in enumerate(classes_list[:2]):\n",
    "            self.__class_to_number[v] = k\n",
    "            self.__number_to_class[k] = v\n",
    "\n",
    "    @staticmethod\n",
    "    def __activation_derivative(neuron_out):\n",
    "        return (1 - neuron_out) * neuron_out\n",
    "\n",
    "    def predict(self, df):\n",
    "\n",
    "        def iterate_over_measures(measure):\n",
    "            result = self.__go_forward(measure)\n",
    "            return self.__number_to_class[int(round(result))]\n",
    "\n",
    "        test_values = self.__build_values(df)\n",
    "        return np.apply_along_axis(iterate_over_measures, 1, test_values)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = LinearPerceptron()\n",
    "model.fit(train_df, features[:100], \"class\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "    1:1:1  1:1:2  1:1:3  1:2:1  1:2:2  1:2:3  1:3:1  1:3:2  1:3:3  1:4:1  ...  \\\n5   196.0  207.0  131.0  194.0  205.0  129.0  190.0  200.0  127.0  186.0  ...   \n10  212.0  217.0  161.0  214.0  219.0  163.0  212.0  220.0  163.0  204.0  ...   \n17   66.0  105.0   86.0   64.0  103.0   84.0   62.0  101.0   82.0   59.0  ...   \n19   55.0   87.0   82.0   54.0   86.0   81.0   53.0   85.0   80.0   53.0  ...   \n\n    50:48:1  50:48:2  50:48:3  50:49:1  50:49:2  50:49:3  50:50:1  50:50:2  \\\n5     174.0    186.0    114.0    174.0    192.0    116.0    174.0    192.0   \n10    146.0    190.0    103.0    150.0    194.0    105.0    150.0    194.0   \n17    153.0    173.0     88.0    120.0    143.0     99.0     87.0    110.0   \n19    178.0    196.0    114.0    173.0    192.0    113.0    158.0    177.0   \n\n    50:50:3  class  \n5     116.0  field  \n10    105.0  field  \n17     66.0  water  \n19     98.0  water  \n\n[4 rows x 7501 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1:1:1</th>\n      <th>1:1:2</th>\n      <th>1:1:3</th>\n      <th>1:2:1</th>\n      <th>1:2:2</th>\n      <th>1:2:3</th>\n      <th>1:3:1</th>\n      <th>1:3:2</th>\n      <th>1:3:3</th>\n      <th>1:4:1</th>\n      <th>...</th>\n      <th>50:48:1</th>\n      <th>50:48:2</th>\n      <th>50:48:3</th>\n      <th>50:49:1</th>\n      <th>50:49:2</th>\n      <th>50:49:3</th>\n      <th>50:50:1</th>\n      <th>50:50:2</th>\n      <th>50:50:3</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>196.0</td>\n      <td>207.0</td>\n      <td>131.0</td>\n      <td>194.0</td>\n      <td>205.0</td>\n      <td>129.0</td>\n      <td>190.0</td>\n      <td>200.0</td>\n      <td>127.0</td>\n      <td>186.0</td>\n      <td>...</td>\n      <td>174.0</td>\n      <td>186.0</td>\n      <td>114.0</td>\n      <td>174.0</td>\n      <td>192.0</td>\n      <td>116.0</td>\n      <td>174.0</td>\n      <td>192.0</td>\n      <td>116.0</td>\n      <td>field</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>212.0</td>\n      <td>217.0</td>\n      <td>161.0</td>\n      <td>214.0</td>\n      <td>219.0</td>\n      <td>163.0</td>\n      <td>212.0</td>\n      <td>220.0</td>\n      <td>163.0</td>\n      <td>204.0</td>\n      <td>...</td>\n      <td>146.0</td>\n      <td>190.0</td>\n      <td>103.0</td>\n      <td>150.0</td>\n      <td>194.0</td>\n      <td>105.0</td>\n      <td>150.0</td>\n      <td>194.0</td>\n      <td>105.0</td>\n      <td>field</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>66.0</td>\n      <td>105.0</td>\n      <td>86.0</td>\n      <td>64.0</td>\n      <td>103.0</td>\n      <td>84.0</td>\n      <td>62.0</td>\n      <td>101.0</td>\n      <td>82.0</td>\n      <td>59.0</td>\n      <td>...</td>\n      <td>153.0</td>\n      <td>173.0</td>\n      <td>88.0</td>\n      <td>120.0</td>\n      <td>143.0</td>\n      <td>99.0</td>\n      <td>87.0</td>\n      <td>110.0</td>\n      <td>66.0</td>\n      <td>water</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>55.0</td>\n      <td>87.0</td>\n      <td>82.0</td>\n      <td>54.0</td>\n      <td>86.0</td>\n      <td>81.0</td>\n      <td>53.0</td>\n      <td>85.0</td>\n      <td>80.0</td>\n      <td>53.0</td>\n      <td>...</td>\n      <td>178.0</td>\n      <td>196.0</td>\n      <td>114.0</td>\n      <td>173.0</td>\n      <td>192.0</td>\n      <td>113.0</td>\n      <td>158.0</td>\n      <td>177.0</td>\n      <td>98.0</td>\n      <td>water</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows Ã— 7501 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.5\n"
     ]
    }
   ],
   "source": [
    "real_classes = test_df[\"class\"].values\n",
    "predicted = model.predict(test_df)\n",
    "\n",
    "print(f\"Accuracy is {accuracy_score(real_classes, predicted)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def calc_accuracy(local_features_to_use):\n",
    "    print(f\"Thread for {len(local_features_to_use)} features works\")\n",
    "    local_model = LinearPerceptron()\n",
    "    local_model.fit(train_df, local_features_to_use, \"class\")\n",
    "    local_predicted_classes = local_model.predict(test_df)\n",
    "    local_acc = round(accuracy_score(real_classes, local_predicted_classes), 3)\n",
    "    return len(local_features_to_use), local_acc\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread for 7500 features works\n",
      "Thread for 7400 features works\n",
      "Thread for 7300 features worksThread for 7200 features works\n",
      "Thread for 7100 features works\n",
      "\n",
      "Thread for 7000 features works\n",
      "Thread for 6900 features works\n",
      "Thread for 6800 features works\n",
      "Thread for 6700 features works\n",
      "Thread for 6600 features works\n",
      "Thread for 6500 features works\n",
      "Thread for 6400 features works\n",
      "Thread for 6300 features works\n",
      "Thread for 6200 features works\n",
      "Thread for 6100 features works\n",
      "Thread for 6000 features works\n",
      "Thread for 5900 features works\n",
      "Thread for 5800 features works\n",
      "Thread for 5700 features works\n",
      "Thread for 5600 features works\n",
      "Thread for 5500 features works\n",
      "Thread for 5400 features works\n",
      "Thread for 5300 features works\n",
      "Thread for 5200 features works\n",
      "Thread for 5100 features works\n",
      "Thread for 5000 features works\n",
      "Thread for 4900 features works\n",
      "Thread for 4800 features works\n",
      "Thread for 4700 features works\n",
      "Thread for 4600 features works\n",
      "Thread for 4500 features works\n",
      "Thread for 4400 features works\n",
      "Thread for 4300 features works\n",
      "Thread for 4200 features works\n",
      "Thread for 4100 features works\n",
      "Thread for 4000 features works\n",
      "Thread for 3900 features works\n",
      "Thread for 3800 features works\n",
      "Thread for 3700 features works\n",
      "Thread for 3600 features works\n",
      "Thread for 3500 features works\n",
      "Thread for 3400 features works\n",
      "Thread for 3300 features works\n",
      "Thread for 3200 features works\n",
      "Thread for 3100 features works\n",
      "Thread for 3000 features works\n",
      "Thread for 2900 features works\n",
      "Thread for 2800 features works\n",
      "Thread for 2700 features works\n",
      "Thread for 2600 features works\n",
      "Thread for 2500 features works\n",
      "Thread for 2400 features works\n",
      "Thread for 2300 features works\n",
      "Thread for 2200 features works\n",
      "Thread for 2100 features works\n",
      "Thread for 2000 features works\n",
      "Thread for 1900 features works\n",
      "Thread for 1800 features works\n",
      "Thread for 1700 features works\n",
      "Thread for 1600 features works\n",
      "Thread for 1500 features works\n",
      "Thread for 1400 features works\n",
      "Thread for 1300 features works\n",
      "Thread for 1200 features works\n",
      "Thread for 1100 features works\n",
      "Thread for 1000 features works\n",
      "Thread for 900 features works\n",
      "Thread for 800 features works\n",
      "Thread for 700 features works\n",
      "Thread for 600 features works\n",
      "Thread for 500 features works\n",
      "Thread for 400 features works\n",
      "Thread for 300 features works\n",
      "Thread for 200 features works\n",
      "50.0% accuracy score with 7500 features\n",
      "50.0% accuracy score with 7400 features\n",
      "50.0% accuracy score with 7300 features\n",
      "50.0% accuracy score with 7200 features\n",
      "50.0% accuracy score with 7100 features\n",
      "50.0% accuracy score with 7000 features\n",
      "50.0% accuracy score with 6900 features\n",
      "50.0% accuracy score with 6800 features\n",
      "50.0% accuracy score with 6700 features\n",
      "50.0% accuracy score with 6600 features\n",
      "50.0% accuracy score with 6500 features\n",
      "50.0% accuracy score with 6400 features\n",
      "50.0% accuracy score with 6300 features\n",
      "50.0% accuracy score with 6200 features\n",
      "50.0% accuracy score with 6100 features\n",
      "50.0% accuracy score with 6000 features\n",
      "50.0% accuracy score with 5900 features\n",
      "50.0% accuracy score with 5800 features\n",
      "50.0% accuracy score with 5700 features\n",
      "50.0% accuracy score with 5600 features\n",
      "50.0% accuracy score with 5500 features\n",
      "50.0% accuracy score with 5400 features\n",
      "50.0% accuracy score with 5300 features\n",
      "50.0% accuracy score with 5200 features\n",
      "50.0% accuracy score with 5100 features\n",
      "50.0% accuracy score with 5000 features\n",
      "50.0% accuracy score with 4900 features\n",
      "50.0% accuracy score with 4800 features\n",
      "50.0% accuracy score with 4700 features\n",
      "50.0% accuracy score with 4600 features\n",
      "50.0% accuracy score with 4500 features\n",
      "50.0% accuracy score with 4400 features\n",
      "50.0% accuracy score with 4300 features\n",
      "50.0% accuracy score with 4200 features\n",
      "50.0% accuracy score with 4100 features\n",
      "50.0% accuracy score with 4000 features\n",
      "50.0% accuracy score with 3900 features\n",
      "50.0% accuracy score with 3800 features\n",
      "50.0% accuracy score with 3700 features\n",
      "50.0% accuracy score with 3600 features\n",
      "50.0% accuracy score with 3500 features\n",
      "50.0% accuracy score with 3400 features\n",
      "50.0% accuracy score with 3300 features\n",
      "50.0% accuracy score with 3200 features\n",
      "50.0% accuracy score with 3100 features\n",
      "50.0% accuracy score with 3000 features\n",
      "50.0% accuracy score with 2900 features\n",
      "50.0% accuracy score with 2800 features\n",
      "50.0% accuracy score with 2700 features\n",
      "50.0% accuracy score with 2600 features\n",
      "50.0% accuracy score with 2500 features\n",
      "50.0% accuracy score with 2400 features\n",
      "50.0% accuracy score with 2300 features\n",
      "50.0% accuracy score with 2200 features\n",
      "50.0% accuracy score with 2100 features\n",
      "50.0% accuracy score with 2000 features\n",
      "50.0% accuracy score with 1900 features\n",
      "50.0% accuracy score with 1800 features\n",
      "50.0% accuracy score with 1700 features\n",
      "50.0% accuracy score with 1600 features\n",
      "50.0% accuracy score with 1500 features\n",
      "50.0% accuracy score with 1400 features\n",
      "50.0% accuracy score with 1300 features\n",
      "50.0% accuracy score with 1200 features\n",
      "50.0% accuracy score with 1100 features\n",
      "50.0% accuracy score with 1000 features\n",
      "50.0% accuracy score with 900 features\n",
      "50.0% accuracy score with 800 features\n",
      "50.0% accuracy score with 700 features\n",
      "50.0% accuracy score with 600 features\n",
      "50.0% accuracy score with 500 features\n",
      "50.0% accuracy score with 400 features\n",
      "50.0% accuracy score with 300 features\n",
      "50.0% accuracy score with 200 features\n",
      "CPU times: user 2h 5min 44s, sys: 24min 15s, total: 2h 30min\n",
      "Wall time: 1h 24min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_variants = (features[:i] for i in range(7500, 100, -100))\n",
    "\n",
    "with ThreadPoolExecutor(5) as executor:\n",
    "    results = executor.map(calc_accuracy, features_variants)\n",
    "for result in results:\n",
    "    f_n, l_c = result\n",
    "    print(f\"{l_c * 100}% accuracy score with {f_n} features\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}